{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vector Analysis\n",
    "March 05, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Introduction\n",
    "\n",
    "This notebook trains a Gensim Word2Vec model using 266 issues of the journal *Stone: An Illustrated Magazine.* It then uses the model to query a series of keywords to find what words were used in similar context to those keywords within the corpus.\n",
    "\n",
    "In addition to a model using the full corpus I made two additional models by splitting up my corpus to see if I could identify changes over time. The first sub-corpus consists of all issues from 1888 through 1910. The second sub-corpus includes all issues after 1910. Due to the missing issues in the 1890s-1900s both sub-corpora consisted of approximately the same number of texts (there are only two additional issues in the post-1910 corpus). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from glob import iglob\n",
    "import gensim\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.manifold import MDS\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.manifold import MDS, TSNE\n",
    "%pylab inline\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Text Model\n",
    "Create a model using the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder_path = os.path.join(os.getcwd(), \"data\")\n",
    "\n",
    "issue_list = []\n",
    "\n",
    "for filename in iglob(os.path.join(data_folder_path, '*.txt')):\n",
    "    \n",
    "    with open(filename) as file_in:\n",
    "        this_issue = file_in.read()\n",
    "    \n",
    "    # Add text as single string to master list\n",
    "    issue_list.append(this_issue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_list[0][8600:9000] #Testing to see if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(issue_list) #Here I am verifying it is picking up all the issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the text in all the issues lower case\n",
    "issues_lower = []\n",
    "for issue in issue_list:\n",
    "    issues_lower.append(issue.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing hyphenated words that were appearing in the model and replacing them with full words.\n",
    "\n",
    "replacements = [\n",
    "    # find -> replace\n",
    "    ('mar-\\nket', \"market\"),\n",
    "    ('vein-\\ning', 'veining'),\n",
    "    (\"effi-\\nciency\", \"efficiency\"),\n",
    "    (\"en-\\ngine\", \"engine\"),\n",
    "    (\"acci-\\ndent\", \"accident\"),\n",
    "    (\"explo-\\nsives\", \"explosives\"),\n",
    "    (\"econ-\\nomy\", \"economy\"),\n",
    "    (\"regu-\\nlations\", \"regulations\")\n",
    "    ]\n",
    "\n",
    "issues_cleaned = []\n",
    "\n",
    "for issue in issues_lower:\n",
    "    for rep, new in replacements:\n",
    "        issue = issue.replace(rep, new)\n",
    "    issues_cleaned.append(issue)\n",
    "                          \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issues_cleaned[0][8600:9000] #Testing to see if it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting each issue into sentences using NLTK's \"sent_tokenize\" function.\n",
    "sentences = [sentence for issue in issues_cleaned for sentence in sent_tokenize(issue)]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Tokenizer to prepare text for processing by Word2Vec model\n",
    "\n",
    "def fast_tokenize(text):\n",
    "    \"\"\"\n",
    "    A version of this function was written by Dr. Laura Nelson and provided to her \"Analyzing Complex Digitized Data\" class /\n",
    "    in Fall 2020 for easy text pre-processing. It takes each sentence, removes punctuation, /\n",
    "    and then turns each sentence into a list of words.\n",
    "    \n",
    "    Input: text string\n",
    "    Output: list of words in string processed to remove punctuation\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get a list of punctuation marks\n",
    "    from string import punctuation\n",
    "    \n",
    "    \n",
    "    # Iterate through text removing punctuation characters\n",
    "    no_punct = \"\".join([char for char in text if char not in punctuation])\n",
    "    \n",
    "    \n",
    "    # Split text over whitespace into list of words\n",
    "    tokens = no_punct.split()\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_sentence = [fast_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_by_sentence = [sentence for sentence in words_by_sentence if sentence != []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test to see if it is working by asking for a random sentence\n",
    "words_by_sentence[700]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "I played around with the values in the model in my investigations, but ultimately used default values as there was little difference between my results.\n",
    "\n",
    "Model value meanings (from Dr. Laura Nelson's Word2Vec class tutorial for \"Analyzing Complex Digitized Data,\" Fall 2020):\n",
    "\n",
    "- Size: Number of dimensions for word embedding model\n",
    "- Window: Number of context words to observe in each direction\n",
    "- min_count: Minimum frequency for words included in model\n",
    "- sg: whether it is a \"Skip-Gram\" or \"Continuous Bag of Words Model\": '1' indicates Skip-Gram\n",
    "- Alpha: Learning rate (initial); prevents model from over-correcting, enables finer tuning\n",
    "- Iterations: Number of passes through dataset\n",
    "- Batch Size: Number of words to sample from data during each pass\n",
    "\n",
    "I used the same process to create two smaller models -- one containing all the issues through 1910 and one containing all issues from 1911 to 1922. I separated out the text files into two separate folders and then directed the code to each of these folders in turn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(words_by_sentence, size=100, window=5, \\\n",
    "                               min_count=40, sg=1, alpha=0.025, iter=5, batch_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save current model for later use\n",
    "\n",
    "model.wv.save_word2vec_format('resources/word2vec.stonejournal-alltext.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading in the models\n",
    "Here I am loading the saved models into this Jupyter notebook rather so that I don't have to go through the code to create each within this notebook every time I use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Full corpus model\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.stonejournal-alltext.txt')\n",
    "\n",
    "#smaller models with pre and post 1910 issues\n",
    "to1910_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.stonejournal-to1910.txt')\n",
    "post1910_model = gensim.models.KeyedVectors.load_word2vec_format('resources/word2vec.stonejournal-post1910.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector-Space Operations - Full Corpus\n",
    "First I stared with a basic investigation several of my health and safety keywords to see what similar words turned up in different scenarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('safety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combining safety and safe\n",
    "model.most_similar('safety', 'safe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar(positive=['health'], negative=['cost'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the tokens\n",
    "\n",
    "An example of a visualization with vocabulary generated from health-related keyword terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_tokens = [token for token,weight in model.most_similar(positive=['health',], topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "health_tokens[:20] #print out top 20 results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create graph\n",
    "vectors = [model[word] for word in health_tokens]\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(health_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt to Introduce Time into Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I'm interested to see if language around safety changed over time I performed some of the same keyword investigations on my two smaller models: one of 1910 and pre-1910 issues (the to1910_model) and one of post-1910 issues (the post1910_model).  Each of these models has approximately 20 million \"words.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to1910_model.most_similar('safety')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post1910_model.most_similar('safety')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to1910_health_tokens = [token for token,weight in to1910_model.most_similar(positive=['health',], topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to1910_health_tokens[:20] #print top 20 results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post1910_health_tokens = [token for token,weight in post1910_model.most_similar(positive=['health',], topn=50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post1910_health_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [model[word] for word in to1910_health_tokens]\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(to1910_health_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [model[word] for word in post1910_health_tokens]\n",
    "dist_matrix = pairwise.pairwise_distances(vectors, metric='cosine')\n",
    "mds = MDS(n_components = 2, dissimilarity='precomputed')\n",
    "embeddings = mds.fit_transform(dist_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(figsize=(10,10))\n",
    "ax.scatter(embeddings[:,0], embeddings[:,1], alpha=0)\n",
    "for i in range(len(vectors)):\n",
    "    ax.annotate(post1910_health_tokens[i], ((embeddings[i,0], embeddings[i,1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
